

# 1. Pose and Velocity Estimation

1. [Optical Flow](https://docs.px4.io/main/en/sensor/optical_flow.html) provides 2D velocity estimation (using a downward facing camera and a downward facing distance sensor)
2. [Visual Inertial Odometry (VIO)](https://docs.px4.io/main/en/computer_vision/visual_inertial_odometry.html) provides 3D pose and velocity estimation using an onboard vision system and IMU. It is used for navigation when global position information is absent or unreliable.

# 2. Collision Avoidance + Path Planning

1. [Collision Prevention](https://docs.px4.io/main/en/computer_vision/collision_prevention.html) is used to stop vehicles before they can crash into an obstacle (primarily when flying in manual modes).

# 3. SLAM with D435i

1. Check out [this video](https://www.youtube.com/watch?v=tcJHnHpwCXk&ab_channel=IntelRealSense) where the source code can bee found [here](https://github.com/IntelRealSense/realsense-ros/wiki/SLAM-with-D435i) 
	1. Problem: This is for ROS1 not ROS2. Can this be converted?
2. Only Problem:
	1. Doesn't do path planning and collision avoidance


# 4.  SLAM with OAK-D cameras

1. Check out [this link](https://docs.luxonis.com/software/ros/vio-slam#VIO%20and%20SLAM-RAE%20on-device%20VIO%20%26%20SLAM) 
2. Also Check out this [RTABMAP on Pi4](https://www.instructables.com/RGB-D-SLAM-With-Kinect-on-Raspberry-Pi-4-Buster-RO/) , but running on ROS1
3. 



# 99. Strategy for implementation:

1. Use SLAM with D435i section and get slam to work
	1. used [this link](https://github.com/simonbogh/realsense-d435-rtab-map-in-ROS2/tree/main) but use humble, not foxy when installing dependencies
	2. Launch using:
```Shell
ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true pointcloud.enable:=true depth_module.depth_profile:=848x480x30

```
or
```Shell
ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true pointcloud.enable:=true

```
and in another terminal:
```Shell
ros2 launch rtabmap_launch rtabmap.launch.py \
 args:="--delete_db_on_start" \
 depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
 rgb_topic:=/camera/camera/color/image_raw \
 camera_info_topic:=/camera/camera/color/camera_info \
 approx_sync:=false \
 frame_id:=camera_link

```


	3. Works, but is very laggy. doubt that  it will work onn drone RBPi

	4. ToDo:
		1. Check if SLAM with D435i can be converted to ros2
		2. Try Orbslam3 with stereo-inertial/depth inertial
		3. Try other VIO SLAM for Ros2. Maybe Kimera or similar
1. Check if this can work with collision avoidance from PX4
2. Find a way to perform path planning and deploy aswell
3. 
















# Visual Based vs Lidar SLAM (Reword)

Visual SLAM relies on cameras to capture images of the environment, which are
then processed using computer vision algorithms to extract features. Laser SLAM
uses LiDAR sensors to capture 3D point clouds of the environment and estimate the
robot position and orientation.

![[Pasted image 20240801084818.png]]

Therefore decided on visual.


# Different SLAM Algorithms

![[Pasted image 20240801085448.png]]

Therefore mainly look at:
1. RTab_Map
	- https://github.com/introlab/rtabmap
	- 
1. ORB_SLAM3
	- https://github.com/UZ-SLAMLab/ORB_SLAM3 ("We have tested the library in **Ubuntu 16.04** and **18.04**, but it should be easy to compile in other platforms. A powerful computer (e.g. i7 **(written in 2021)** ) will ensure real-time performance and provide more stable and accurate results"). - **Might still be worth a try**
	- https://github.com/suchetanrs/ORB-SLAM3-ROS2-Docker
	- 
1. Elastic Fusion("A very fast nVidia GPU 3.5TFLOPS+, and a fast CPU (something like an i7 **(written in 2016)** "). -Doubt it, but possibly worth a try
	- https://github.com/FengyuGuo/ElasticFusionRos
	- 
Then look at:
1. RGBD-SLAM-v2 (apparently not for ARM and needs GPU to "Detected keypoints using SIFT (GPU)")
	-  https://github.com/felixendres/rgbdslam_v2
	- 
1. OKVIS
	- https://github.com/ethz-asl/okvis
	- 
1. DVO
	- https://github.com/tum-vision/dvo
	- 


















# 1. Pose and Velocity Estimation

1. [Optical Flow](https://docs.px4.io/main/en/sensor/optical_flow.html) provides 2D velocity estimation (using a downward facing camera and a downward facing distance sensor)
2. [Visual Inertial Odometry (VIO)](https://docs.px4.io/main/en/computer_vision/visual_inertial_odometry.html) provides 3D pose and velocity estimation using an onboard vision system and IMU. It is used for navigation when global position information is absent or unreliable.

# 2. Collision Avoidance + Path Planning

1. [Collision Prevention](https://docs.px4.io/main/en/computer_vision/collision_prevention.html) is used to stop vehicles before they can crash into an obstacle (primarily when flying in manual modes).

# 3. SLAM with D435i

1. Check out [this video](https://www.youtube.com/watch?v=tcJHnHpwCXk&ab_channel=IntelRealSense) where the source code can bee found [here](https://github.com/IntelRealSense/realsense-ros/wiki/SLAM-with-D435i) 
	1. Problem: This is for ROS1 not ROS2. Can this be converted?
2. Only Problem:
	1. Doesn't do path planning and collision avoidance


# 4.  SLAM with OAK-D cameras

1. Check out [this link](https://docs.luxonis.com/software/ros/vio-slam#VIO%20and%20SLAM-RAE%20on-device%20VIO%20%26%20SLAM) 
2. 



# 99. Strategy for implementation:

1. Use SLAM with D435i section and get slam to work
	1. used [this link](https://github.com/simonbogh/realsense-d435-rtab-map-in-ROS2/tree/main) but use humble, not foxy when installing dependencies
	2. Launch using:
```Shell
ros2 launch rtabmap_launch rtabmap.launch.py \
 args:="--delete_db_on_start" \
 depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
 rgb_topic:=/camera/camera/color/image_raw \
 camera_info_topic:=/camera/camera/color/camera_info \
 approx_sync:=false \
 frame_id:=camera_link

```


	3. Works, but is very laggy. doubt that  it will work onn drone RBPi

	4. ToDo:
		1. Check if SLAM with D435i can be converted to ros2
		2. Try Orbslam3 with stereo-inertial/depth inertial
		3. Try other VIO SLAM for Ros2. Maybe Kimera or similar
1. Check if this can work with collision avoidance from PX4
2. Find a way to perform path planning and deploy aswell
3. 
















# Visual Based vs Lidar SLAM (Reword)

Visual SLAM relies on cameras to capture images of the environment, which are
then processed using computer vision algorithms to extract features. Laser SLAM
uses LiDAR sensors to capture 3D point clouds of the environment and estimate the
robot position and orientation.

![[Pasted image 20240801084818.png]]

Therefore decided on visual.


# Different SLAM Algorithms

![[Pasted image 20240801085448.png]]

Therefore mainly look at:
1. RTab_Map
	- https://github.com/introlab/rtabmap
	- 
1. ORB_SLAM3
	- https://github.com/UZ-SLAMLab/ORB_SLAM3
	- https://github.com/suchetanrs/ORB-SLAM3-ROS2-Docker
	- 
1. Elastic Fusion
	- https://github.com/FengyuGuo/ElasticFusionRos
	- 
Then look at:
1. RGBD-SLAM-v2
	-  https://github.com/felixendres/rgbdslam_v2
	- 
1. OKVIS
	- https://github.com/ethz-asl/okvis
	- 
1. DVO
	- https://github.com/tum-vision/dvo
	- 














